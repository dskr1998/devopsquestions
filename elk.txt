1.why elk is used ?
  ans:-in microservice application to monitor,debug and analyze the performance of the application we need to use elastic stack.
note:- microservices running across the cluster generates logs and placed across the pods distributed on the workernode of the                cluster.
2.how to identify for a given request which microservices are involved in processing and what are their outcomes of execution,incase
  of failure how to debug and analyze the root cause of the failure ?
  ans:-to help us in tracking and analyzing the logs for a given request the development team has include a unique number which is 
	called traceid or flowid or correlation id etc as part of the applications and propagates the unique number for that request
	across the  microservices.and this unique number or traceid would be used for logging messages in the logs that are generated
	by the microservice applications.
	now the support/ops/devops/developers and teams has to search through the logs to identify request patterns,failure patterns 
	and other aspects of analyzing the application.
	but the problem is logs are distributed/scattered across the pods/nodes of the cluster,it will be difficult to go through each
	node of the cluster and search for logs to analyze the information.
	to aggregate all the logs and browse/analyze easily ELK stack has beed been introduced.
3.elk stands for ?
  ans:- elasticsearch,logstash and kibana.these are the 3 components used for aggregating the logs generated across the nodes of the
	cluster and read/process them easily through kibana.
elasticSearch
-------------
in a logfile we want to search the information based on 
1.errors
2.keywords / common attributes / fields of the application like
  traceid,loggedInUser,Date&Time,Index,Application Name
3.free form text  (anything)
4.regular expression (matching)

the application logs files accumulates and aggregates over a period of time into GB's,searching through such huge log files and 
idenifying the required information takes huge cpu,memory and tools like notepad++, text editors becomes unresponsive.

to support searching through large volumes of log files and identify and return search results elasticsearch is used.the logs that are
generated by the application are pushed into elasticsearch engine.

what is an index ?
  ans:-For each application representing the logs of that application we create an index in elasticsearch engine, per each logfile we        define fields or attributes to be indexed

       so that elastic search is an clustered search engine that can search for the log messages based on these indexed fields and        returns the results quickly

logstash
--------
LogStash is an forwarder agent/tool that will reads the log files and messages generated from the these log files and formats them and converts them into elasticsearch format and publishes them to the elasticsearch engine.

Kibana/Splunk/Graphana
----------------------
Kibana is an data visualization tool, the pulls the logs information from elasticsearch and displays log messages based on the search patterns.
We can create dashboards and display metrics information interms of charts, graphs helping us in monitoring and analyzing the health of the application

The major problem with this architecture is:
The logstash has to be executed as an sidecar container in each microservice pod, that is going to consume considerable amount of cpu/memory in analyzing, formatting and publishing the logs to the elasticsearch engine. Due to this the resource consumption in a pod increases. The performance of the application will be impacted and unnecessary pod migrations will be observed

To overcome the above problem, elasticstack has introduced beats components which are light weight forwarders. There are different beat components are there
1. filebeat
2. metricsbeat
3. auditbeat
4. packetbeat
5. heartbeat
etc

these are called datashippers which ships the data to the logstash. If we use along with ELK the beats components in implementing the ELK then it is called Elastic Stack


what is sidecar container ?
  ans:- running the logstash within the same pod of the application.(it is design pattern in kubernetes).is called sidecar container.
why do we need to run the logstash on the samepod ?
  ans:- otherwise it will not read the logs from the applications.(otherwise we need to run the logstash in each node by using         deployment).
what is logstash ?
  ans:- logstash is an utility/tool provided by elastic search team that reads the logs messages from the physical locations and         uploads/publishes to elastic search engine.
what is kibana ?
  ans:- kibana is an dashboard application that can read the logs from elastic search based on search patterns and renders the logs in
	the UI.
using kibana what we can generate ?
  ans:- 1.graphs
	2.charts
	to analyze the metrics.we can configure alerts based on patterns.
how application and logstash will communicate with eachother ?
  ans:- using localhost.
incase the application container is died then what about the logstash ?
  ans:- logstash also will destroyed.
how are you implementing elk ?
  ans:- we use aws elastic search engine.(opensearch in aws).

**(application container * logstash) -->  (clusterip) elastic search

all the logstash will talk to static clusterIP  through which we publish the logs.
kibana also will talk to the logstash through clusterIP.
(kibana will expose to the public through ingress) 




in which order the application should run which are implemented elk ?
  ans:- mysql along with elastic search --> microservices --> log stash -->kibana dash board.

note:- ci directory placed with project root directory.

docker-compose
|-compose.yml
  |-container 
and required which are required.

how to run docker-compose ?
  ans:- docker-compose -d up
 after run this command it will go to compose.yml file read the containers and their order and apply it.

how to install docker compose ?
  ans:- docker compose is an utility that gets install when we install docker.

docker-compose.yaml
-------------------
version: '3.8'     (docker compose version)
services: 	   (which containers we are running)
  mysqldb:	   (service name)(insted of container ip we can use service name)
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: 'root'
    ports:
      - '3306:3306'
    volumes:
      - "./scripts/baedal-sql.sql:/docker-entrypoint-initdb.d/init.sql"   (any thing that is placed -entrypoint-initdb.d/init.sql) 
    healthcheck:							   (will auto executed by the container)
      test: timeout 5 bash -c 'cat </dev/null > /dev/tcp/localhost/33060'
      interval: 5s
      timeout: 5s
      retries: 12
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.2.3
    environment:
      - xpack.security.enable				(when we are sending the request to the elastic search we dont need to pass
      - "discovery.type=single				(username and password)  (we  are runnig the elastic search in one node)
      - "ES_JAVA_OPTS=-Xms750
    ports:
      - 9200:9200					(port for elastic search)
      - 9300:9300
    healthcheck:
      test:
        [
          "CMD-SHELL",
	  "curl http://localhost:9200 | grep -q 'you know, for Search'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
  kibana:
    image: "docker.elastic.co/kibana/kibana:8.2.3"        (kibana docker image)
    environment:
      SERVERNAME: kibana
      ELASTICSEARCH_HOSTS: '["http://elasticsearch:9200"]' 	(it will read the messages from elastic search)
      xpack.security.enabled: 'false'
    ports:
      - "5601:5601"			(kibana expose port)
    depends_on:				(when does the kibana container should start  means depends on elastic search engine)
      elasticsearch:
        condition: service_healthy	(and elastic search healthy is up and start then start kibana) 
  logstash:
    image: "docker.elastic.co/logstash/logstash:8.2.3"		(logstash image)
    volumes:
      - ./elksettings/:/etc/logstash
      - '/var/log/baedal:/var/log/baedal/'	(all the microservices are writing the data /var/log/baedal)
    command: logstash -f /etc/logstash/logstash.conf
    depends_on:					 
      elasticsearch:
        condition: service_healthy
  user-mgmt-service:
    image: techsriman/usermgmtservice:1.0
    environment:
      SPRING_DATASOURCE_URL: 'jdbc:mysql://mysqldb:3306/baedalusermgmtdb'
      SPRING_DATASOURCE_USERNAME: 'root'
      SPRING_DATASOURCE_PASSWORD: 'root'
      SPRING_DATASOURCE_HIKARI_MAXIMUM-POOL-SIZE: 10
      USERMGMTSERVICE_TEMPLATES_DIR: '/templates'
      LOGGING_LOGBACK_ROLLINGPOLICY_MAX-FILE-SIZE: 500MB
    depends_on:
      elasticsearch:
        condition: service_healthy
      mysqldb:
	condition: service_healthy
    ports:
      - 9090:9090
    volumes:
      - './templates:/templates'
      - './var/log/baedal:/var/log/baedal/'
  pickup-delivery-mgmt-service:
    image: techsriman/pickupdeliverymgmtservice: 1.0
    environment:
      SPRING_DATASOURCE_URL: 'jdbc:mysql://mysqldb:3306/baedalpickupanddeliverymgmtdb'
      SPRING_DATASOURCE_USERNAME: 'root'
      SPRING_DATASOURCE_PASSWORD: 'root'
      SPRING_DATASOURCE_HIKARI_MAXIMUM-POOL-SIZE: 10
      USERMGMTSERVICE_TEMPLATES_DIR: '/templates'
      LOGGING_LOGBACK_ROLLINGPOLICY_MAX-FILE-SIZE: 500MB      
    depends_on:
      elasticsearch:
        condition: service_healthy
      mysqldb:
	condition: service_healthy
    ports:
      - 9092:9092
    volumes:
      - './templates:/templates'
      - '/var/log/baedal:/var/log/baedal/'
  partner-store-service:
    image: techsriman/partnerstoreservice:1.0
    environment:
      SPRING_DATASOURCE_URL: 'jdbc:mysql://mysqldb:3306/baedalpartnerstoredb'
      SPRING_DATASOURCE_USERNAME: 'root'
      SPRING_DATASOURCE_PASSWORD: 'root'
      SPRING_DATASOURCE_HIKARI_MAXIMUM-POOL-SIZE: 10
      USERMGMTSERVICE_TEMPLATES_DIR: '/templates'
      LOGGING_LOGBACK_ROLLINGPOLICY_MAX-FILE-SIZE: 500MB
    depends_on:
      elasticsearch:
	condition: service_healthy
      mysqldb:
	condition: service_healthy
      ports:
	- 9091:9091
      volumes:
        - './templates:/templates'
        - '/var/log/baedal:/var/log/baedal/'
  baedalcustomer:
    image: techsriman/baedalcustomer:1.0
    environment:
      USERACCOUNTSERVICE_URL: 'http://user-mgmt-service:9090'
      PARTNERSTORESERVICE_URL: 'http://partner-store-service:9091'
      PICKUPANDDELIVERYSERVICE_URL: 'http://pickup-delivery-mgmt-service:9092'
      USERMGMTSERVICE_TEMPLATES_DIR: '/templates'
      BAEDAL_DEFAULT_URL: 'http://localhost:8082'
      LOGGING_LOGBACK_ROLLINGPOLICY_MAX-FILE-SIZE: 500MB
    depends_on:
      elasticsearch:
        condition: service_healthy
      mysqldb:
	condition: service_healthy
      user-mgmt-service:
	condition: service_started
      partner-store-service:
	condition: service_started
      pickup-delivery-mgmt-service:
	condition: service_started
    ports:
      - 8082:8082
    volumes:
      - '/var/log/baedal:/var/log/baedal/'



1.how to run docker compose file ?
  ans:- docker-compose up -d  (means run all the containers in background).
2.how to see the containers which we run by using docker-compose ?
  ans:- docker-compose -ps
3.when we run docker-compose -ps what it will show ?
  ans:- it will show name,command,state,ports
4.how to see the logs of the container ?
  ans:- docker container logs containername.
5.when we create the containers using docker compose how the network connection happens ?
  ans:- docker compose automatically takecare of networking of the containers.
note:-
  application generated logs --> elastic search --> (browse them using kibana).
6.where do we need to see the logs in kibana dashboard ?
  ans:-index
	 goto stackmanagement --> index management -->    (or)
	goto hamburger -- discover     here we can search for the logs in search bar not in discover bar.
7.how to search for specific word in logs ?
  ans:- example like error.
	message:error   (body of the message that conatains error it will show).(means it will show error word that are there in logs)
8.we can search for the logs last 15 minutes (or) last one hour (or) from last month like..
9.what are the available fields that are there in kibana dashboard by using them we can search for the logs ?
  ans:- @timestamp,@version,event original,_score,_index,hostname,_id,message,log file path.










































































































































































































 